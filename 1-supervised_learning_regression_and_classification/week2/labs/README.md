# 📘Week 2: Linear Regression with Multiple Variables
## 🔍Course: Supervised Machine Learning - Regression and Classification
Instructor: Andrew Ng
Platform: Coursera (via DeepLearning.AI)

### 🧠Topics Covered:

- Multiple Features (Multivariate Linear Regression)

- Notation for multiple variables

- Feature Engineering

- Feature Scaling (Normalization and Standardization)

- Vectorization for efficient computation

- Gradient Descent with multiple variables

### 📈Key Concepts:

- Multivariable Linear Regression: Extending linear regression to use multiple features for better prediction accuracy.

- Feature Engineering: Creating new input features that help the model learn more effectively.

- Feature Scaling:

- Ensures features contribute equally to the cost function.

- Helps gradient descent converge faster.

- Vectorization: Using matrix operations to optimize computations, especially for large datasets.

### 💻What I Did:

- Implemented multivariable linear regression from scratch

- Scaled features using standardization

- Rewrote gradient descent with vectorized implementation

- Compared model performance with and without feature scaling

### 🔧Tools & Skills Used:

- Python

- NumPy

- Jupyter Notebook

- Linear Algebra for ML

