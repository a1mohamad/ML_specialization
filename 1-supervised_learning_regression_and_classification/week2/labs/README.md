# ğŸ“˜Week 2: Linear Regression with Multiple Variables
## ğŸ”Course: Supervised Machine Learning - Regression and Classification
Instructor: Andrew Ng
Platform: Coursera (via DeepLearning.AI)

### ğŸ§ Topics Covered:

- Multiple Features (Multivariate Linear Regression)

- Notation for multiple variables

- Feature Engineering

- Feature Scaling (Normalization and Standardization)

- Vectorization for efficient computation

- Gradient Descent with multiple variables

### ğŸ“ˆKey Concepts:

- Multivariable Linear Regression: Extending linear regression to use multiple features for better prediction accuracy.

- Feature Engineering: Creating new input features that help the model learn more effectively.

- Feature Scaling:

- Ensures features contribute equally to the cost function.

- Helps gradient descent converge faster.

- Vectorization: Using matrix operations to optimize computations, especially for large datasets.

### ğŸ’»What I Did:

- Implemented multivariable linear regression from scratch

- Scaled features using standardization

- Rewrote gradient descent with vectorized implementation

- Compared model performance with and without feature scaling

### ğŸ”§Tools & Skills Used:

- Python

- NumPy

- Jupyter Notebook

- Linear Algebra for ML

