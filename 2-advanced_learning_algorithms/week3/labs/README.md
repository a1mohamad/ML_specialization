# Week 3: Neural Networks - Representation

Welcome to **Week 3** of Course 2 in the [Machine Learning Specialization by Andrew Ng](https://www.coursera.org/specializations/machine-learning-introduction)!  
This week focuses on the foundational concepts of neural networks, including their architecture and how they represent complex functions.

---

## 📚 Topics Covered

- Neural network architecture (input layer, hidden layers, output layer)
- Activation functions (sigmoid, ReLU, tanh)
- Forward propagation
- Computing the output of a neural network
- Representation power of neural networks
- Notation and conventions for deep networks

---

## 📝 Learning Objectives

By the end of this week, you should be able to:

- Describe the structure of a neural network and the role of each layer
- Implement the forward propagation algorithm
- Explain how neural networks can approximate complex functions
- Use vectorized implementations for efficient computation in neural networks

---

## 🗂️ This Directory Contains

- **Lecture notes**: Summaries and key takeaways from the video lectures.
- **Jupyter notebooks**: Code examples and exercises to reinforce your understanding.
- **Assignments**: Programming assignments and solutions for hands-on practice.
- **Resources**: Additional readings and references.

---

## 🚀 Getting Started

1. Clone this repository:
   ```bash
   git clone https://github.com/a1mohamad/ML-specialization.git
   ```
2. Navigate to this week’s folder:
   ```bash
   cd ML-specialization/course-2/week3
   ```
3. Open the Jupyter notebooks to begin exploring the materials.

---

## 📌 Recommended Prerequisites

- Familiarity with Python and NumPy and Pandas
- Understanding of logistic regression and basic linear algebra
- TensorFlow
---

## 🤝 Acknowledgments

Content and assignments in this folder are based on materials from the [Machine Learning Specialization by Andrew Ng](https://www.deeplearning.ai/courses/machine-learning-specialization/).

---

Happy Learning! 🚀
