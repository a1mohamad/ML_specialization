# Week 3: Neural Networks - Representation

Welcome to **Week 3** of Course 2 in the [Machine Learning Specialization by Andrew Ng](https://www.coursera.org/specializations/machine-learning-introduction)!  
This week focuses on the foundational concepts of neural networks, including their architecture and how they represent complex functions.

---

## ğŸ“š Topics Covered

- Neural network architecture (input layer, hidden layers, output layer)
- Activation functions (sigmoid, ReLU, tanh)
- Forward propagation
- Computing the output of a neural network
- Representation power of neural networks
- Notation and conventions for deep networks

---

## ğŸ“ Learning Objectives

By the end of this week, you should be able to:

- Describe the structure of a neural network and the role of each layer
- Implement the forward propagation algorithm
- Explain how neural networks can approximate complex functions
- Use vectorized implementations for efficient computation in neural networks

---

## ğŸ—‚ï¸ This Directory Contains

- **Lecture notes**: Summaries and key takeaways from the video lectures.
- **Jupyter notebooks**: Code examples and exercises to reinforce your understanding.
- **Assignments**: Programming assignments and solutions for hands-on practice.
- **Resources**: Additional readings and references.

---

## ğŸš€ Getting Started

1. Clone this repository:
   ```bash
   git clone https://github.com/a1mohamad/ML-specialization.git
   ```
2. Navigate to this weekâ€™s folder:
   ```bash
   cd ML-specialization/course-2/week3
   ```
3. Open the Jupyter notebooks to begin exploring the materials.

---

## ğŸ“Œ Recommended Prerequisites

- Familiarity with Python and NumPy and Pandas
- Understanding of logistic regression and basic linear algebra
- TensorFlow
---

## ğŸ¤ Acknowledgments

Content and assignments in this folder are based on materials from the [Machine Learning Specialization by Andrew Ng](https://www.deeplearning.ai/courses/machine-learning-specialization/).

---

Happy Learning! ğŸš€
